{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import openai.error\n",
    "from tqdm import tqdm\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs clean up\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "pattern = r'results-row\\d+-row\\d+.csv'\n",
    "\n",
    "file_list = []\n",
    "\n",
    "directory = 'E:\\\\Coding\\\\Python\\\\Jupyter Notebook'\n",
    "\n",
    "def list_files():\n",
    "  global file_list\n",
    "  file_list = []\n",
    "  for filename in os.listdir(directory):\n",
    "      # print(filename)\n",
    "      # if re.match(pattern, filename):\n",
    "        # print('match found', filename)\n",
    "      if not os.path.isfile(os.path.join(directory, filename)):\n",
    "        continue\n",
    "      if re.match(pattern, filename):\n",
    "        file_list.append(filename)\n",
    "      if filename == \"latest_j.txt\":\n",
    "        file_list.append(filename)\n",
    "\n",
    "def cleanup():\n",
    "  # s = input(\"type 'yes' if you want to remove output file: \")\n",
    "  # if s == 'yes':\n",
    "    print(file_list)\n",
    "    try: \n",
    "      if len(file_list):\n",
    "        s = input(\"Found older result files, cleanup? y/n: \")\n",
    "        if s == 'y':\n",
    "          for filename in file_list:\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "          return 1\n",
    "        else: \n",
    "          print('cool')\n",
    "          return 0\n",
    "    except FileNotFoundError:\n",
    "      print(\"File not present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup logger for debugging\n",
    "## Don't touch\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.FileHandler('debug.log', mode='w')\n",
    "handler.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handle rate limit error with exponential backoff strategy\n",
    "\n",
    "import random\n",
    "import time\n",
    " \n",
    "def retry_with_exponential_backoff(\n",
    "    func,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    errors: tuple = (openai.error.RateLimitError, openai.error.ServiceUnavailableError),\n",
    "):\n",
    " \n",
    "    def wrapper(*args, **kwargs):\n",
    "        num_retries = 0\n",
    "        delay = initial_delay\n",
    " \n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    " \n",
    "            except errors as e:\n",
    "                logger.exception(e)\n",
    "                num_retries += 1\n",
    " \n",
    "                if num_retries > max_retries:\n",
    "                    raise Exception(\n",
    "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                    )\n",
    " \n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    " \n",
    "                time.sleep(delay)\n",
    " \n",
    "            except Exception as e:\n",
    "                logger.exception(e)\n",
    "                raise e\n",
    " \n",
    "    return wrapper\n",
    "    \n",
    "@retry_with_exponential_backoff\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "# Please upload your input CSV file and paste the path down here (starts with /content)\n",
    "input_csv = \"E:\\\\Coding\\\\Python\\\\Jupyter Notebook\\\\input.csv\"\n",
    "# from and to row, set it to -1,-1 if you want to run it for the whole csv\n",
    "from_row = -1\n",
    "to_row = -1\n",
    "# batch size\n",
    "batch_length = 20\n",
    "# for naming of file\n",
    "FROM, TO = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell after setting up the prompt context and prompt question\n",
    "\n",
    "prompt_context = f'''\n",
    "Assist me in researching website niches using a list of URLs. For each URL, provide the niche and whether the site is in English or non-English. Use the format \"URL - Niche, Language\". Select one niche from the list and indicate the language as either English or non-English, e.g., \"Advertising, non-English\".\n",
    "\n",
    "\n",
    "\n",
    "Academia, College & University\n",
    "Advertising\n",
    "Animals\n",
    "Animation\n",
    "Apparel & Fashion\n",
    "Architecture\n",
    "Artificial intelligence\n",
    "Arts & Culture\n",
    "Astronomy\n",
    "Athlete\n",
    "Automotive & Cars\n",
    "Aviation\n",
    "Badminton\n",
    "Baseball\n",
    "Basketball\n",
    "Beauty & Cosmetics\n",
    "Biking\n",
    "Biotechnology\n",
    "Blogging\n",
    "Boating\n",
    "Books, Reading & Magazine\n",
    "Camping\n",
    "Cats\n",
    "CBD & Cannabis\n",
    "Children, Infants & Baby\n",
    "Civil Rights & Social Action\n",
    "Climbing\n",
    "Coaching\n",
    "Community Organization\n",
    "Computers\n",
    "Consumer Internet\n",
    "Cooking\n",
    "Crafts\n",
    "Cricket\n",
    "Cruises\n",
    "Cycling & Mountain Biking\n",
    "Dancing\n",
    "Design\n",
    "Diet\n",
    "DIY\n",
    "Dogs\n",
    "E-Commerce & Business\n",
    "Economics\n",
    "Editing\n",
    "Education, Teaching & E-learning\n",
    "Electronics\n",
    "Entertainment\n",
    "Entrepreneurship\n",
    "Events\n",
    "Film\n",
    "Finance & Financial Services\n",
    "Fishing\n",
    "Fitness, Exercise & Bodybuilding\n",
    "Food & Beverage\n",
    "Football\n",
    "Fragrance\n",
    "Furniture\n",
    "Gadgets\n",
    "Gardening\n",
    "Golf\n",
    "Haircare\n",
    "Health Care & Mental Health\n",
    "Health, Wellness & Holistic\n",
    "Hiking\n",
    "Hockey\n",
    "Home Improvement\n",
    "Hospitality\n",
    "Human Resources\n",
    "Human Rights\n",
    "Illustration\n",
    "Interior Design\n",
    "Internet\n",
    "Internet Marketing\n",
    "Investing\n",
    "Journalism\n",
    "Kid's / Child's Fashion\n",
    "Kids\n",
    "Learning\n",
    "Lifestyle\n",
    "Literature\n",
    "Local Business\n",
    "Luxury goods, Jewellery & Jewelry\n",
    "Marketing\n",
    "Martial Arts\n",
    "Medicine, Nutrition, Supplements & Vitamins\n",
    "Meditation\n",
    "Men's Fashion\n",
    "Men's Health\n",
    "Men's Interest\n",
    "Music\n",
    "Nature\n",
    "News\n",
    "Non-Profit Organization\n",
    "Outdoors & Adventure\n",
    "Painting\n",
    "Parenting\n",
    "Performing Arts\n",
    "Personal Development\n",
    "Personal Finance\n",
    "Pets\n",
    "Photography\n",
    "Politics\n",
    "Psychology\n",
    "Public Figure & Celebrity\n",
    "Publishing\n",
    "Real Estate\n",
    "Recruiting\n",
    "Religious Organization\n",
    "Restaurants\n",
    "Retail\n",
    "Running\n",
    "SaaS\n",
    "Science & Technology\n",
    "Seniors\n",
    "Shoes & Footwear\n",
    "Skateboarding\n",
    "Skiing\n",
    "Skincare & Makeup\n",
    "Snowboarding\n",
    "Soccer\n",
    "Social Commerce\n",
    "Social Media\n",
    "Social Services\n",
    "Software\n",
    "Software Development\n",
    "Sports\n",
    "Startups\n",
    "Sustainable living\n",
    "Swimming\n",
    "Technology\n",
    "Tennis\n",
    "Theatre\n",
    "Travel, Leisure & Tourism\n",
    "Utilities, Services & Telecommunications\n",
    "Venture Capital\n",
    "Video Games & Gaming\n",
    "Weddings\n",
    "Women's Fashion\n",
    "Women's Health\n",
    "Women's Interest\n",
    "Writing\n",
    "Yoga\n",
    "\n",
    "'''\n",
    "\n",
    "prompt_question = f\"What all niches do these {batch_length} websites belong to from my list and are they in English or non-english:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latest_j.txt', 'results-row0-row100.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 5/5 [01:31<00:00, 18.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to start running\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "try:\n",
    "  list_files()\n",
    "  flag = cleanup()\n",
    "except FileNotFoundError as e:\n",
    "  print(\"File not found, make sure directory path is correctly is setup\")\n",
    "  logger.error(\"File not found, make sure directory path is correctly is setup\")\n",
    "except Exception as e:\n",
    "  print(\"Some error occured in cleanup\")\n",
    "  logger.exception(\"Exception: \")\n",
    "\n",
    "try:\n",
    "  df = pd.read_csv(input_csv, header=None)\n",
    "except FileNotFoundError:\n",
    "  print(\"File not found, Re-upload and try again\")\n",
    "  logger.exception(\"Exception: \")\n",
    "  sys.exit()\n",
    "except:\n",
    "  print(\"Some error occured in reading file\")\n",
    "  logger.exception(\"Exception: \")\n",
    "  sys.exit()\n",
    "\n",
    "df_length = len(df.index)\n",
    "\n",
    "if from_row == -1 or flag:\n",
    "  from_row, to_row = [0, df_length]\n",
    "  FROM, TO = from_row, to_row\n",
    "\n",
    "try:\n",
    "  website_names = df.iloc[from_row : to_row, 0]\n",
    "  if(len(website_names) < batch_length):\n",
    "    batch_length = len(website_names)\n",
    "except:\n",
    "  print(f\"Error reading input, running for all rows\")\n",
    "  logger.exception(\"Exception: \")\n",
    "  website_names = df.iloc[:, 0]\n",
    "\n",
    "\n",
    "total_websites = len(website_names)\n",
    "\n",
    "results = []\n",
    "\n",
    "j = from_row\n",
    "\n",
    "if not flag:\n",
    "  try:\n",
    "    with open('latest_j.txt', 'r') as f:\n",
    "      j = int(f.read())\n",
    "      from_row = j\n",
    "      logger.debug(\"from_row: \", from_row)\n",
    "  except FileNotFoundError:\n",
    "    print(\"Running for the first time\")\n",
    "    logger.exception(\"Exception: \")\n",
    "  except:\n",
    "    print(\"Some error occured in reading j value, try again, starting from 0\")\n",
    "    logger.exception(\"Exception: \")\n",
    "\n",
    "\n",
    "try:\n",
    "    progress_bar = tqdm(range(from_row, to_row, batch_length), desc=\"Processing batches\")\n",
    "    for i in progress_bar:\n",
    "      #print('i: ', i)\n",
    "      website_batch = df.iloc[i : i + batch_length, 0]\n",
    "      # print('website_batch_length: ', len(website_batch))\n",
    "      completion = completions_with_backoff(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": f\"{prompt_context}\"},\n",
    "          {\"role\": \"user\", \"content\": f\"{prompt_question} {website_batch}\"}\n",
    "      ],\n",
    "      n = 1,\n",
    "      temperature = 0.1,\n",
    "      max_tokens = 2048\n",
    ")\n",
    "      # print(completion)\n",
    "      response = completion.choices[0].message.content\n",
    "      # print(response)\n",
    "      websites_and_niches = response.split('\\n')\n",
    "      # print(websites_and_niches)\n",
    "\n",
    "      # print('website_batch: ', website_batch)\n",
    "      #print('j: ', j)\n",
    "      for website_and_niche in websites_and_niches:\n",
    "        # if len(website_and_niche) and website_and_niche[0].isdigit():\n",
    "        if not len(website_and_niche):\n",
    "          continue\n",
    "        if(website_and_niche.find(' - ') == -1):\n",
    "          continue\n",
    "        website_and_niche_list = website_and_niche.split(' - ')\n",
    "        try:\n",
    "          website = website_batch[j]\n",
    "        except (KeyError, ValueError):\n",
    "          #print(\"f error: \", j, i)\n",
    "          j=i\n",
    "          logger.exception(\"Exception: \")\n",
    "          logger.debug(\"f error: j, i = \", j, i)\n",
    "          website = website_batch[j]\n",
    "        # print(\"website: \", website)\n",
    "        niches = website_and_niche_list[1]\n",
    "        # print(website, niches)\n",
    "        niche_list = niches.split(', ')\n",
    "        # print(website, niche_list)\n",
    "        result = [website, *niche_list]\n",
    "        result_df = pd.DataFrame([result])\n",
    "        result_df.to_csv(f'results-row{FROM}-row{TO}.csv', mode='a', index=False, header=False)\n",
    "        # results.append(result)\n",
    "        with open('latest_j.txt', 'w') as f:\n",
    "          j += 1\n",
    "          # print(\"j: \", j)\n",
    "          f.write(str(j))\n",
    "      #progress_bar.set_description(f\"Total {i }\")\n",
    "except Exception as e:\n",
    "  with open('latest_j.txt', \"w\") as f:\n",
    "    f.write(str(j))\n",
    "  print(\"Some error occured, for more details, check logs, or, to resume re-run cell\")\n",
    "  print(\"Error: \", type(e), e)\n",
    "  logger.exception(\"Exception: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing area for multithreading, this is in beta, do not run\n",
    "\n",
    "import pandas as pd\n",
    "import concurrent.futures as cf\n",
    "\n",
    "df = pd.read_csv('/content/Copy_of_The_Sun_US_the-sun.com_batch_4_-_March_20_5_20_PM_-_The_Sun_US_the-sun.com_batch_4.csv')\n",
    "\n",
    "total_websites_to_test = 1800\n",
    "max_threads = 20\n",
    "\n",
    "websites = df.iloc[:total_websites_to_test, 0]\n",
    "\n",
    "batch_length = 20\n",
    "\n",
    "def make_req(website_batch):\n",
    "  completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"What all niches do these {batch_length} websites {website_batch} belong to?\"}\n",
    "      ],\n",
    "      n=1,\n",
    "      temperature=0.1,\n",
    "      max_tokens=2048\n",
    "  )\n",
    "  response = completion.choices[0].message.content\n",
    "  # print(response)\n",
    "  return response\n",
    "\n",
    "with cf.ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "  future_to_batch = {executor.submit(make_req, websites[i : i + batch_length]): i for i in range(0, total_websites_to_test, batch_length)}\n",
    "  for future in cf.as_completed(future_to_batch):\n",
    "    response = future_to_batch[future]\n",
    "    try:\n",
    "      data = future.result()\n",
    "    except Exception as exc:\n",
    "      print(f\"{response} generated the error {exc}\")\n",
    "    else:\n",
    "      print(data)\n",
    "\n",
    "\n",
    "\n",
    "# make_req(websites[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
